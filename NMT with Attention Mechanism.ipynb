{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/spa.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import string\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\nimport re\nimport os\nimport io\nimport time","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the text file of Spanish-English pairs:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"lines=pd.read_table(\"/kaggle/input/spa.txt\",names=['input','target','comments'])\nlines.head(10)","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"  input    target                                           comments\n0   Go.       Ve.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n1   Go.     Vete.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n2   Go.     Vaya.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n3   Go.   Váyase.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n4   Hi.     Hola.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n5  Run!   ¡Corre!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n6  Run!  ¡Corran!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n7  Run!   ¡Corra!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n8  Run!  ¡Corred!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n9  Run.   Corred.  CC-BY 2.0 (France) Attribution: tatoeba.org #4...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>target</th>\n      <th>comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Ve.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Go.</td>\n      <td>Vete.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Go.</td>\n      <td>Vaya.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Go.</td>\n      <td>Váyase.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hi.</td>\n      <td>Hola.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Run!</td>\n      <td>¡Corre!</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Run!</td>\n      <td>¡Corran!</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Run!</td>\n      <td>¡Corra!</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Run!</td>\n      <td>¡Corred!</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Run.</td>\n      <td>Corred.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines=lines[['input','target']]\nlines","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                                                    input  \\\n0                                                     Go.   \n1                                                     Go.   \n2                                                     Go.   \n3                                                     Go.   \n4                                                     Hi.   \n...                                                   ...   \n124320  There are four main causes of alcohol-related ...   \n124321  There are mothers and fathers who will lie awa...   \n124322  A carbon footprint is the amount of carbon dio...   \n124323  Since there are usually multiple websites on a...   \n124324  If you want to sound like a native speaker, yo...   \n\n                                                   target  \n0                                                     Ve.  \n1                                                   Vete.  \n2                                                   Vaya.  \n3                                                 Váyase.  \n4                                                   Hola.  \n...                                                   ...  \n124320  Hay cuatro causas principales de muertes relac...  \n124321  Hay madres y padres que se quedan despiertos d...  \n124322  Una huella de carbono es la cantidad de contam...  \n124323  Como suele haber varias páginas web sobre cual...  \n124324  Si quieres sonar como un hablante nativo, debe...  \n\n[124325 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Ve.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Go.</td>\n      <td>Vete.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Go.</td>\n      <td>Vaya.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Go.</td>\n      <td>Váyase.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hi.</td>\n      <td>Hola.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>124320</th>\n      <td>There are four main causes of alcohol-related ...</td>\n      <td>Hay cuatro causas principales de muertes relac...</td>\n    </tr>\n    <tr>\n      <th>124321</th>\n      <td>There are mothers and fathers who will lie awa...</td>\n      <td>Hay madres y padres que se quedan despiertos d...</td>\n    </tr>\n    <tr>\n      <th>124322</th>\n      <td>A carbon footprint is the amount of carbon dio...</td>\n      <td>Una huella de carbono es la cantidad de contam...</td>\n    </tr>\n    <tr>\n      <th>124323</th>\n      <td>Since there are usually multiple websites on a...</td>\n      <td>Como suele haber varias páginas web sobre cual...</td>\n    </tr>\n    <tr>\n      <th>124324</th>\n      <td>If you want to sound like a native speaker, yo...</td>\n      <td>Si quieres sonar como un hablante nativo, debe...</td>\n    </tr>\n  </tbody>\n</table>\n<p>124325 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# We have separated the English and Spanish Sentences.."},{"metadata":{"trusted":true},"cell_type":"code","source":"lines.sample(5)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"                                                   input  \\\n98091           The seventh day of the week is Saturday.   \n42601                          I told you I'm not ready.   \n11095                                  I don't know you.   \n14120                                 I hope she's safe.   \n111671  May I talk with you in private about the matter?   \n\n                                            target  \n98091    El séptimo día de la semana es el sábado.  \n42601                  Te dije que no estoy listo.  \n11095                               No te conozco.  \n14120                     Espero que esté a salvo.  \n111671  ¿Puedo hablarte a solas acerca del asunto?  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>98091</th>\n      <td>The seventh day of the week is Saturday.</td>\n      <td>El séptimo día de la semana es el sábado.</td>\n    </tr>\n    <tr>\n      <th>42601</th>\n      <td>I told you I'm not ready.</td>\n      <td>Te dije que no estoy listo.</td>\n    </tr>\n    <tr>\n      <th>11095</th>\n      <td>I don't know you.</td>\n      <td>No te conozco.</td>\n    </tr>\n    <tr>\n      <th>14120</th>\n      <td>I hope she's safe.</td>\n      <td>Espero que esté a salvo.</td>\n    </tr>\n    <tr>\n      <th>111671</th>\n      <td>May I talk with you in private about the matter?</td>\n      <td>¿Puedo hablarte a solas acerca del asunto?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Now preprocessing the statements:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_sentence(sentence):\n    \n    num_digits= str.maketrans('','', digits)\n    \n    sentence= sentence.lower()\n    sentence= re.sub(\" +\", \" \", sentence)\n    sentence= re.sub(\"'\", '', sentence)\n    sentence= sentence.translate(num_digits)\n    sentence= re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n    sentence = sentence.rstrip().strip()\n    sentence=  'START_ ' + sentence + ' _END'\n    \n    return sentence","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines['input']=lines['input'].apply(preprocess_sentence)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines['input']","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"0                                          START_ go . _END\n1                                          START_ go . _END\n2                                          START_ go . _END\n3                                          START_ go . _END\n4                                          START_ hi . _END\n                                ...                        \n124320    START_ there are four main causes of alcohol-r...\n124321    START_ there are mothers and fathers who will ...\n124322    START_ a carbon footprint is the amount of car...\n124323    START_ since there are usually multiple websit...\n124324    START_ if you want to sound like a native spea...\nName: input, Length: 124325, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines['target']=lines['target'].apply(preprocess_sentence)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines['target']","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"0                                          START_ ve . _END\n1                                        START_ vete . _END\n2                                        START_ vaya . _END\n3                                      START_ váyase . _END\n4                                        START_ hola . _END\n                                ...                        \n124320    START_ hay cuatro causas principales de muerte...\n124321    START_ hay madres y padres que se quedan despi...\n124322    START_ una huella de carbono es la cantidad de...\n124323    START_ como suele haber varias páginas web sob...\n124324    START_ si quieres sonar como un hablante nativ...\nName: target, Length: 124325, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(lines)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"124325"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = lines.to_numpy().tolist()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = np.array(rows)\nrows","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"array([['START_ go . _END', 'START_ ve . _END'],\n       ['START_ go . _END', 'START_ vete . _END'],\n       ['START_ go . _END', 'START_ vaya . _END'],\n       ...,\n       ['START_ a carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities .  some people try to reduce their carbon footprint because they are concerned about climate change . _END',\n        'START_ una huella de carbono es la cantidad de contaminación de dióxido de carbono que producimos como producto de nuestras actividades .  algunas personas intentan reducir su huella de carbono porque están preocupados acerca del cambio climático . _END'],\n       ['START_ since there are usually multiple websites on any given topic ,  i usually just click the back button when i arrive on any webpage that has pop-up advertising .  i just go to the next page found by google and hope for something less irritating . _END',\n        'START_ como suele haber varias páginas web sobre cualquier tema ,  normalmente sólo le doy al botón de retroceso cuando entro en una página web que tiene anuncios en ventanas emergentes .  simplemente voy a la siguiente página encontrada por google y espero encontrar algo menos irritante . _END'],\n       ['START_ if you want to sound like a native speaker ,  you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . _END',\n        'START_ si quieres sonar como un hablante nativo ,  debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . _END']],\n      dtype='<U295')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Made English Spanish Pairs.."},{"metadata":{"trusted":true},"cell_type":"code","source":"english=[]\nfor i in lines['input']:\n    english.append(i)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(english)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"124325"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"spanish=[]\nfor i in lines['target']:\n    spanish.append(i)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(spanish)","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"124325"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Creating the input and target tokens:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\ninput_sentence_tokenizer.fit_on_texts(english)\ninput_array = input_sentence_tokenizer.texts_to_sequences(english)\ninput_array= tf.keras.preprocessing.sequence.pad_sequences(input_array,padding='post' )","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_array","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"array([[    1,    45,     3, ...,     0,     0,     0],\n       [    1,    45,     3, ...,     0,     0,     0],\n       [    1,    45,     3, ...,     0,     0,     0],\n       ...,\n       [    1,     9,  4168, ...,     0,     0,     0],\n       [    1,   382,    49, ...,     0,     0,     0],\n       [    1,    68,     7, ..., 13628,     3,     2]], dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\ntarget_sentence_tokenizer.fit_on_texts(spanish)\ntarget_array = target_sentence_tokenizer.texts_to_sequences(english)\ntarget_array= tf.keras.preprocessing.sequence.pad_sequences(target_array,padding='post',maxlen=30)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_array","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"array([[    1, 20921,     3, ...,     0,     0,     0],\n       [    1, 20921,     3, ...,     0,     0,     0],\n       [    1, 20921,     3, ...,     0,     0,     0],\n       ...,\n       [    1,     7,  4211, ...,     0,     0,     0],\n       [    1,    18, 21985, ...,     0,     0,     0],\n       [    1, 26229,     7, ...,     0,     0,     0]], dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(target_array[0]))","execution_count":22,"outputs":[{"output_type":"stream","text":"30\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_target_length= max(len(t) for t in  target_array)\nprint(max_target_length)\nmax_source_length= max(len(t) for t in  input_array)\nprint(max_source_length)","execution_count":23,"outputs":[{"output_type":"stream","text":"30\n51\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Padding the sentences to a certain length::-"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\ninput_sentence_tokenizer.fit_on_texts(english)\ninput_array = input_sentence_tokenizer.texts_to_sequences(english)\ninput_array= tf.keras.preprocessing.sequence.pad_sequences(input_array,padding='post',maxlen=20)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\ntarget_sentence_tokenizer.fit_on_texts(spanish)\ntarget_array = target_sentence_tokenizer.texts_to_sequences(english)\ntarget_array= tf.keras.preprocessing.sequence.pad_sequences(target_array,padding='post',maxlen=30)","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train, input_val, target_train, target_val = train_test_split(input_array, target_array, test_size=0.2)\n\nprint(len(input_train), len(target_train), len(input_val), len(target_val))","execution_count":26,"outputs":[{"output_type":"stream","text":"99460 99460 24865 24865\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 80-20 Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(input_sentence_tokenizer.word_index)+1)","execution_count":27,"outputs":[{"output_type":"stream","text":"13629\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(target_sentence_tokenizer.word_index)+1)","execution_count":28,"outputs":[{"output_type":"stream","text":"26832\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"When the dataset is big, we want to create the dataset in memory to be efficient. We will use tf.data.Dataset.from_tensor_slices() method to get slices of the array in the form of an object."},{"metadata":{"trusted":true},"cell_type":"code","source":"buffer_size = len(input_train)\nbatch_size = 64\nsteps_per_epoch = len(input_train)//batch_size\nembedding_dim = 256\nunits = 1024\nvocab_input_size = len(input_sentence_tokenizer.word_index)+1\nvocab_target_size = len(target_sentence_tokenizer.word_index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(buffer_size)\ndataset = dataset.batch(batch_size, drop_remainder=True)\n# print(type(dataset))","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoder-Decoder Architecture:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self,vocab_size,embedding_size,units,batchsize):\n        super(Encoder, self).__init__()\n        self.batchsize=batchsize\n        self.units=units\n        self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru=tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True)# entire sequence of outputs will be returned from all the units.\n        #To return the internal state of GRU, we set the return_state to True\n    \n    def call(self,y,hidden):\n        y=self.embedding(y)\n        output,state=self.gru(y,initial_state=hidden)\n        return output,state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batchsize, self.units))","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"(TensorShape([64, 20]), TensorShape([64, 30]))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Encoder(vocab_input_size, embedding_dim, units,batch_size )\n\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))","execution_count":32,"outputs":[{"output_type":"stream","text":"Encoder output shape: (batch size, sequence length, units) (64, 20, 1024)\nEncoder Hidden state shape: (batch size, units) (64, 1024)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Creating a Bahdanau Attention Layer:-"},{"metadata":{},"cell_type":"markdown","source":"Attention layer consists:-\n\nAlignment Score\n\nAttention weights\n\nContext vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention,self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n    \n    # The encoder hiiden states are taken as input to the attention layer which are of shape (batch size, units)\n    # and the encoder output of each timestep is of shape (batch size, sequence length, units).\n    # so for adding we have to expand dimensions\n    def call(self,encoder_out,encoder_hid):\n        \n       hidden1=tf.expand_dims(encoder_hid,1)\n        \n        \n       # score shape == (batch_size, max_length, 1)\n       # we get 1 at the last axis because we are applying score to self.V\n       # the shape of the array before applying self.V is (batch_size, max_length, units)\n       score = self.V(tf.nn.tanh(\n          self.W1(encoder_out) + self.W2(hidden1)))\n\n       # attention_weights shape == (batch_size, max_length, 1)\n       attention_weights = tf.nn.softmax(score, axis=1) ## the alignment scores for each encoder hidden state\n        #are combined and represented in a single vector and subsequently softmaxed\n\n       # context_vector shape after sum == (batch_size, hidden_size)\n       context_vector = attention_weights * encoder_out ## attention weights multiplied with the encoder output states are used to calculate the context vactor\n        \n       context_vector = tf.reduce_sum(context_vector, axis=1)\n\n       return context_vector, attention_weights ## returning the context vector and the attention_weights","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention_layer = Attention(5)# 5 for units of attention\nattention_result, attention_weights = attention_layer(sample_output,sample_hidden)\n\nprint(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\nprint(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))","execution_count":34,"outputs":[{"output_type":"stream","text":"Attention result shape: (batch size, units) (64, 1024)\nAttention weights shape: (batch_size, sequence_length, 1) (64, 20, 1)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# The context vector should be of the shape of (batch size, units) as it be combined with the decoder previous embeddings.."},{"metadata":{},"cell_type":"markdown","source":"# Decoder Class:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self,vocab_size,embedding_size,units,batchsize):\n        super(Decoder, self).__init__()\n        self.batchsize=batchsize\n        self.units=units\n        self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru=tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True)# entire sequence of outputs will be returned from all the units.\n        #To return the internal state of GRU, we set the return_state to True\n        \n        #fully connected layer for the decoder outputs\n        self.fc = tf.keras.layers.Dense(vocab_size)\n        \n        # attention layer\n        self.attention=Attention(self.units)\n        \n    def call(self, x, enc_output,hidden):\n        # enc_output shape == (batch_size, max_length, hidden_size)\n        context_vector, attention_weights = self.attention(enc_output,hidden)\n#         print(context_vector.shape)\n\n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n#         print(x.shape)\n        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)## context vaector is added with the previous decoder hidden state.\n       \n        # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n\n        # output shape == (batch_size * 1, hidden_size)\n        output = tf.reshape(output, (-1, output.shape[2]))\n\n        # output shape == (batch_size, vocab)\n        x = self.fc(output)\n\n        return x, state, attention_weights       ","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(vocab_target_size, embedding_dim, units, batch_size)\n\nsample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)),\n                                      sample_output, sample_hidden)\n\nprint ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))","execution_count":57,"outputs":[{"output_type":"stream","text":"(64, 1024)\n(64, 1, 256)\nDecoder output shape: (batch_size, vocab size) (64, 26832)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Defining the Optimiser and Loss Function:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer=tf.keras.optimizers.RMSprop()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","execution_count":59,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}